#

1. 虚函数和纯虚函数
2. 智能指针
3. 移动语义和完美转发

## 音视频

1. 音视频同步
2. I B P 帧， IBP帧之间的关系
   1. 因为要先解码 B 帧，需要 I & P 帧，如果先传入了 B 帧，仍需等待 P 帧才能解码 B 帧。
   2. PTS >= DTS 因为需要先 decode 才能 present。
3. 264 结构

## 音视频同步

### 为什么音视频同步，大多数要采用音频为主时钟？

1. 一方面考虑到人对声音的敏感度要强于视频，频繁调节音频会带来较差的观感体验，且音频的播放时钟为线性增长，所以一般会以音频时钟为参考时钟，视频同步到音频上。
2. 另一方面，音频的播放和传输相对于视频来说更为简单和稳定。音频数据通常以固定的采样率进行处理，且音频流的传输速度相对较低。这使得音频在播放和同步上更加可靠。

DTS （Decoding timestamp） 解码时间戳 - 意思是帧的解码耗时

PTS （Presentation timestamp） 展示时间戳 - 解码后的帧的原始帧的显示时间

### 常见的同步策略

* **基于时间戳的同步**（Timestamp-based Synchronization）：在编码时，音频和视频帧会被赋予一个时间戳（Timestamp），表示它们应该在什么时候被播放。播放器会根据这些时间戳来播放音视频，从而实现同步。
* **基于缓冲区的同步**（Buffer-based Synchronization）：播放器会为音频和视频各自维护一个缓冲区（Buffer）。当缓冲区中的数据达到一定量时，播放器会开始播放。通过控制缓冲区的大小，可以在一定程度上实现音视频同步。**可以有效地处理网络延迟等问题**
* **基于帧率的同步**（Frame Rate-based Synchronization）：在某些情况下，我们可以通过控制音视频的帧率（Frame Rate）来实现同步。例如，如果视频的帧率是30帧/秒，音频的采样率是48000采样/秒，那么每播放一帧视频，就应该播放1600个音频采样。

### 音频缓冲区和音频设备缓冲区会冲突么？

音频设备内部确实有一个缓冲区，这个缓冲区主要用于存储即将播放的音频数据。当我们将音频数据发送给音频设备时，音频设备会将这些数据存储在其内部的缓冲区中，然后按照一定的速度从缓冲区中取出数据进行播放。

然而，这并不意味着我们不能在音频设备之外再设置一个缓冲区。实际上，音频设备的内部缓冲区和我们设置的外部缓冲区是两个不同的概念，它们各自承担不同的任务。

音频设备的内部缓冲区主要用于保证音频的连续播放，避免因为数据传输延迟而导致的播放中断。而我们设置的外部缓冲区则主要用于控制音频的播放速度，以实现音视频同步。

在音视频同步的过程中，我们可以通过控制外部缓冲区中的数据量，以及数据送入音频设备的速度，来调整音频的播放速度。这种方法并不会与音频设备的内部缓冲区产生冲突，因为这两个缓冲区各自承担不同的任务，而且它们之间的数据传输是有序的：我们首先将数据从外部缓冲区送入音频设备，然后音频设备再从其内部缓冲区中取出数据进行播放。

### 删除帧可以理解，插入额外帧指哪些帧？

插入额外的帧通常是指在视频流中添加一些新的帧，以保持音视频同步。这些额外的帧可以是重复的帧，也可以是通过插值生成的新的帧。

* 重复帧：这是最简单的方法，就是将前一帧或后一帧复制一份作为新的帧。这种方法的优点是实现简单，但缺点是可能会导致视频的播放不流畅，因为用户可能会察觉到帧的重复。
* 插值帧：这是一种更复杂的方法，需要使用一些算法（例如，运动插值算法）来生成新的帧。插值帧是根据前后两帧的内容计算出来的，它代表了前后两帧之间的一个过渡状态。这种方法的优点是可以生成更自然、更流畅的视频，但缺点是实现复杂，计算量大。

在实际的应用中，选择哪种方法主要取决于具体的需求和条件。例如，如果对视频的播放流畅性要求很高，那么可能会选择使用插值帧；反之，如果对实现复杂度和计算量有限制，那么可能会选择使用重复帧。

### 调整帧率具体怎么做，是调整解码器帧率？

调整帧率是另一种常见的音视频同步策略。帧率，也就是每秒钟显示的帧数，是衡量视频流畅度的重要指标。在音视频同步的过程中，我们可以通过调整音频或视频的帧率来达到同步的目的。

例如，假设我们有一个音频流和一个视频流，它们的播放速度不同步。如果视频的帧率比音频的帧率高，那么视频会比音频播放得更快，导致不同步。在这种情况下，我们可以通过减小视频的帧率（也就是减少每秒钟显示的帧数）来降低视频的播放速度，从而使视频与音频同步。

同样，如果音频的播放速度比视频快，我们可以通过增加视频的帧率来提高视频的播放速度，从而使视频与音频同步。

需要注意的是，调整帧率并不一定意味着改变解码器的解码帧率。解码器的解码帧率通常是固定的，由视频的编码格式决定。调整帧率通常是通过丢弃一些帧或插入一些额外的帧来实现的，这不会改变解码器的解码帧率，但会改变最终显示给用户的帧率。

### 如何判断用户无法觉察的时间区间？

用户对于视频流畅性的感知能力会受到多种因素的影响，包括视频的内容、用户的注意力、用户的视觉敏感度等。因此，很难给出一个精确的区间来描述用户能够察觉到的帧插入或删除的数量。然而，我们可以根据一些常见的视觉感知原理，给出一些大致的指导。

* 视觉暂留效应：人眼有一种视觉暂留效应，即在一瞬间看到的图像会在短时间内保留在视网膜上。这个时间通常在100-200毫秒之间。因此，如果在这个时间内插入或删除一帧，用户可能不会察觉到。
* 帧率和视觉流畅性：人眼对于帧率的感知能力有一定的限制。一般来说，当帧率超过24帧/秒时，人眼就会感觉到视频是连续的，而不是由一帧帧单独的图像组成。因此，如果在一秒钟内插入或删除少于24帧，用户可能不会察觉到。
* 内容和注意力：用户对于视频的注意力和视频的内容也会影响他们的感知能力。例如，如果视频的内容变化很快，或者用户的注意力分散，那么他们可能不会察觉到少量的帧插入或删除。

总的来说，如果你想要在用户无法察觉的情况下插入或删除帧，你应该尽量保持插入或删除的帧数较少（例如，每秒钟不超过24帧），并且尽量在用户的视觉暂留效应的时间内（例如，100-200毫秒）进行操作。然而，这只是一个大致的指导，具体的效果可能会因为多种因素而有所不同。

### 缓冲区的数据结构选择？

在音视频处理中，缓冲区通常使用的数据结构是队列或环形缓冲区。这是因为音视频数据的处理通常需要满足先入先出（FIFO）的原则，即先解码的数据先播放，后解码的数据后播放，这与队列和环形缓冲区的特性相符。

* 队列：队列是一种先入先出（FIFO）的数据结构，适合用于存储需要按照特定顺序处理的数据。在音视频处理中，我们可以将解码后的音视频帧存入队列，然后按照顺序从队列中取出帧进行播放。
* 环形缓冲区：环形缓冲区是一种特殊的队列，它的头尾是相连的，形成一个环形。环形缓冲区的优点是可以有效地利用存储空间，当缓冲区满时，新的数据可以覆盖旧的数据。在音视频处理中，如果我们希望在缓冲区满时能够继续存储新的音视频帧，而且不关心被覆盖的旧帧，那么可以使用环形缓冲区。

链表、数组和二叉树等其他数据结构在音视频处理的缓冲区中使用较少。链表和数组在插入和删除数据时的效率较低，而二叉树则更适合用于需要进行快速查找和排序的场景，而不是音视频处理这种需要按照特定顺序处理数据的场景。

### 音视频两者处理速度是不一致的,怎么调整同步?

音频和视频的处理速度确实可能不同，这是因为音频和视频的解码、渲染和播放过程可能需要不同的时间。为了保证音视频同步，你需要在播放每一帧时检查音频和视频的同步状态，并根据需要进行调整。

以下是一些可能的方法：

* 使用系统时钟：你可以使用一个系统时钟来同步音频和视频。系统时钟是一个单调递增的时钟，它表示从播放开始到现在的时间。在播放每一帧时，你可以将这一帧的PTS（Presentation Time Stamp）和系统时钟进行比较，如果PTS比系统时钟快，那么你可以延迟播放这一帧，如果PTS比系统时钟慢，那么你可以尽快播放这一帧。
* 调整播放速度：如果音频和视频的处理速度有较大的差距，你可能需要调整播放速度。例如，如果视频处理速度比音频快，你可以减慢视频的播放速度，或者加快音频的播放速度。
* 使用缓冲区：你可以使用缓冲区来存储解码后的音频和视频帧。在播放每一帧时，你可以检查音频和视频缓冲区的状态，如果一个缓冲区为空，那么你可以等待，直到这个缓冲区有数据可供播放。
* 使用同步机制：在多线程环境中，你可以使用同步机制（如互斥锁、条件变量等）来同步音频和视频的播放。例如，你可以使用一个条件变量来等待音频和视频都处理完毕，然后再播放下一帧。

### 音视频两者样本数是不一样的,怎么进行同步?

你是对的，音频和视频的样本数（或帧数）通常是不一样的。视频通常以固定的帧率（如24帧/秒或30帧/秒）播放，而音频的样本率通常远高于视频的帧率（如44100样本/秒或48000样本/秒）。这意味着在同一秒内，音频的样本数通常远多于视频的帧数。

然而，尽管音频和视频的样本数不一样，但是它们的时间戳（PTS，Presentation Time Stamp）应该是一致的，至少在理想情况下是这样。PTS表示的是这个样本或帧应该在什么时候被播放，而不是这个样本或帧是第几个样本或帧。因此，即使音频的样本数多于视频的帧数，但是它们的PTS应该是一致的。

例如，假设我们有一个视频帧，它的PTS是1.0秒，这意味着这个视频帧应该在播放开始后的1.0秒被显示。同时，我们也有一些音频样本，它们的PTS也是1.0秒，这意味着这些音频样本应该在播放开始后的1.0秒被播放。尽管音频样本的数量可能远多于视频帧的数量，但是它们的PTS是一致的。

在实际的音视频同步中，你需要确保音频和视频的PTS一致，或者至少在一个可接受的范围内。如果音频和视频的PTS有较大的差距，那么你可能需要进行一些调整，例如调整播放速度，或者延迟其中一个流的播放，以使它们的PTS一致。

### 是否一般以秒为单位计算同步区间?

是的，通常情况下，音频和视频的时间戳（PTS，Presentation Time Stamp）都是以秒为单位的。这意味着，如果你有一个时间戳为1.0的帧，那么这个帧应该在播放开始后的1.0秒被播放。

在播放音频和视频时，你需要根据当前的系统时间和每一帧的PTS来决定何时播放这一帧。例如，如果当前的系统时间是1.0秒，那么你应该播放所有PTS小于或等于1.0秒的帧。

## 编码流程

* 打开视频文件，
* avio_open 打开输出文件
* avformat_new_stream 创建H264视频流，并设置参数
* avcodec_find_encoder 查找编码器
* avcodec_alloc_context3  ---AVCodecContext  设置编码器内容
  * 设置编码器的一系列参数
* avcodec_open2  打开编码器
* 读取每一帧YUV,将其转化为h264
* 编码
  * avcodec_send_frame 往 AVCodecContext 编码器 发送一个 AVFrame。
  * avcodec_receive_packet 从 AVCodecContext 编码器 读取一个 AVPacket。
* 释放内存

## 解码流程

* avformat_alloc_context 分配一个 AVFormatContext
* avformat_open_input 打开输入文件
* avformat_find_stream_info  获取视频流信息
* avcodec_find_decoder 查找解码器
* avcodec_alloc_context3  根据解码器参数来创建解码器内容---AVCodecContext
* avcodec_open2 打开解码器
* av_read_frame 读取AVPacket
* avcodec_send_packet 往 AVCodecContext 解码器 发送一个 AVPacket 。
* avcodec_receive_frame 从 AVCodecContext 解码器 读取一个 AVFrame。
* 将解码的数据写为420yuv

  ```cpp
    fwrite(yuvFrame->data[0],1,w*h,fp);//y
                        fwrite(yuvFrame->data[1],1,w*h/4,fp);//u
                        fwrite(yuvFrame->data[2],1,w*h/4,fp);//v
   ```

## H.264 与 H.265 有什么区别？

1）主要区别

* H.265 也称为高效视频编码 (HEVC)，是 H.264 的升级和更高级的版本；
* H.265 的编码架构大致上 和 H.264 的架构相似，主要也包含：帧内预测（intra prediction）、帧间预测（inter prediction）、转换（transform）、量化（quantization）、去区块滤波器（deblocking filter）、熵编码（entropy coding）等模块。但在 H.265 编码架构中，整体被分为了三个基本单位，分别是编码单位（coding unit, CU）、预测单位（predict unit, PU）和转换单位（transform unit, TU）；
* 比起 H.264，H.265 提供了更多不同的工具来降低码率，以编码单位来说，H.264 中每个宏块（macroblock/MB）大小最大为 16x16 像素，而 H.265 的编码单位最大为 64x64；
* H.265 的帧内预测模式支持 35 种方向（而 H.264 只支持 8 种），并且提供了更好的运动补偿处理和矢量预测方法。
