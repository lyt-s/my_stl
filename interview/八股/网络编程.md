# 网络

<https://www.cnblogs.com/Hijack-you/p/13057792.html>

select 底层实现是数组，poll底层实现是链表。
select 最大连接数是 1024(X64 2048)
poll 无上限 65535
epoll 无上限 65535
![s](./assets/select_poll_epoll.png)

三者性能差异：

1. 事件发生时，是否遍历
2. 用户态往内核空间的拷贝，select poll 需要进行遍历查询就绪事件，将就绪事件返回给用户程序
3. 活动连接比较多时，epoll_wait未必会比select和poll高，因为此时触发回调过于频繁，所以epoll_wait适用于连接数量多，但活动连接较少的情况。

## select为什么只能支持1024个。poll和epoll是怎么解决这个问题的?

## epoll底层数据结构

![alt text](./assets/epoll底层数据结构.png)

![alt text](./assets/epoll_creat.png)

eventpoll本身其实是一个fd,文件类型是匿名文件。
四个核心字段：

1. 等待队列，等待队列头，有那些线程是在等待我这个事件发生的
2. 第二个rdlist,有多少个就绪的，需要拷贝给应用上层的。
3. 红黑树的节点。
4. 指向一个真正的文件类型的指针。
![alt text](./assets/evectpoll.png)

![alt text](./assets/创建eventpoll.png)

### epoll_create

![alt text](./assets/epitem.png)

![alt text](./assets/rbtree.png)

![alt text](./assets/filefd.png)

1. file 指针和fd在运行过程中是不会变的。比较起来也非常快。
2. **就没有用到红黑树有序的特性。** **他不需要保证有序，只需要保证快速的建树，快速的做比较，做增删改查就可以了。**

![alt text](./assets/image.png)

![alt text](./assets/image2.png)

### epoll_ctl

![alt text](./assets/image3.png)
![alt text](./assets/image4.png)
![alt text](./assets/image5.png)
![alt text](./assets/image6.png)

![alt text](./assets/image7.png)
![alt text](./assets/image8.png)

### epoll_wait

#### 怎么阻塞自己？

把当前线程加到eventpoll 的等待队列上，让出了CPU。
![alt text](./assets/image9.png)
![alt text](./assets/image10.png)
![alt text](./assets/image11.png)
![alt text](./assets/image12.png)
![alt text](./assets/image13.png)

![alt text](./assets/image14.png)

#### 网络事件是如何通知到用户进程的？

软中端， gdb

## epoll的红黑树是如何挂载事件？

调用epoll_create时，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件。

当epoll_wait调用时，仅仅观察这个双向链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。而且，通常情况下即使我们要监控百万计的句柄，大多一次也只返回很少量的准备就绪句柄而已，所以，epoll_wait仅需要从内核态copy少量的句柄到用户态而已。

## 红黑树和就绪队列的关系？

**红黑树的结点和就绪队列的结点的同一个节点，所谓的加入就绪队列，就是将结点的前后指针联系到一起。所以就绪了不是将红黑树结点delete掉然后加入队列。他们是同一个结点，不需要delete。**

## epoll 红黑树

为什么采用红黑树呢？因为和epoll的工作机制有关。epoll在添加一个socket或者删除一个socket或者修改一个socket的时候，它需要查询速度更快，操作效率最高，因此需要一个更加优秀的数据结构能够管理这些socket。

我们想到的比如链表，数组，二叉搜索树，B+树等都无法满足要求，
• 因为链表在查询，删除的时候毫无疑问时间复杂度是O(n)；
• 数组查询很快，但是删除和新增时间复杂度是O(n)；
• 二叉搜索树虽然查询效率是lgn，但是如果不是平衡的，那么就会退化为线性查找，复杂度直接来到O(n)；
• B+树是平衡多路查找树，主要是通过降低树的高度来存储上亿级别的数据，但是它的应用场景是内存放不下的时候能够用最少的IO访问次数从磁盘获取数据。比如数据库聚簇索引，成百上千万的数据内存无法满足查找就需要到内存查找，而因为B+树层高很低，只需要几次磁盘IO就能获取数据到内存，所以在这种磁盘到内存访问上B+树更适合。

**为了均衡，啥都想要(查找，插入，更小的内存开销)**
因为我们处理上万级的fd，它们本身的存储空间并不会很大，所以倾向于在内存中去实现管理，而红黑树是一种非常优秀的平衡树，它完全是在内存中操作，而且查找，删除和新增时间复杂度都是lgn，效率非常高，因此选择用红黑树实现epoll是最佳的选择。

### 为什么不选择AVL树(map)？

当然不选择用AVL树是因为红黑树是不符合AVL树的平衡条件的，**红黑是用非严格的平衡来换取增删节点时候旋转次数的降低，任何不平衡都会在三次旋转之内解决**；而AVL树是严格平衡树，在增加或者删除节点的时候，根据不同情况，旋转的次数比红黑树要多。所以红黑树的插入效率更高。

### 为什么不选择哈希？

但是在我们调用epoll_create()的时候，hash底层的数组创建多大合适呢？

**如果我们有百万的fd，那么这个数组越大越好，如果我们仅仅十几个fd需要管理，在创建数组的时候，太大的空间就很浪费。而这个fd我们又不能预先知道有多少，所以hash是不合适的。**

哈希表作为一种常见的数据结构，用于存储键值对。但是在高并发环境下，哈希表可能会面临以下性能问题：

* 哈希冲突（Collisions）：冲突指的是两个或更多的键映射到了同一个桶（bucket）。在高并发情况下，由于并发写入，冲突可能会更加频繁。
* 并发安全性（Concurrency Safety）：在大规模并发情况下，多个线程或进程同时对哈希表进行读写操作，可能导致数据一致性问题或竞态条件。
* 扩容（Resize）：在哈希表中插入或删除大量的键值对时，可能会导致哈希表的负载因子增加，从而影响哈希表的性能。
* 在大规模并发情况下，读取性能可能成为一个瓶颈。

### 为什么不选择B+树？

b/b+tree是多叉树，一个结点可以存多个key，主要是用于降低层高，用于磁盘索引的，所以在我们这个内存场景下也是不适合的。

### 就绪socket列表-双向链表

就绪列表存储的是就绪的socket，所以它应能够快速的插入数据。

程序可能随时调用epoll_ctl添加监视socket，也可能随时删除。当删除时，若该socket已经存放在就绪列表中，它也应该被移除。（事实上，每个epoll_item既是红黑树节点，也是链表节点，删除红黑树节点，自然删除了链表节点）

所以就绪列表应是一种能够快速插入和删除的数据结构。双向链表就是这样一种数据结构，epoll使用双向链表来实现就绪队列（rdllist）

### 等待队列

在我们执行，epoll_wait时，实际就是将当前进程加入到设备fd的等待队列上。

![alt text](./assets/等待队列.png)

### 网络编程释疑之：TCP半开连接的处理

### 网络编程UDP如何处理丢包和包的顺序

## selected

![alt text](./assets/select.png)
三个位图。 写读异常

### 普通文件可以使用epoll/select/poll来监听文件变化吗？

不可以

1. 普通文件是一直可写的。事件会一直回调。

vfs 虚拟文件系统  

## Linux I/O

### 缓存I/O和直接I/O的区别？

**缓存I/O又被称作标准I/O**，大多数文件系统的默认I/O操作都是缓存I/O。在Linux的缓存I/O机制中，数据先从磁盘复制到内核空间的缓冲区，然后从内核空间缓冲区复制到应用程序的地址空间。

**缓存I/O的优点：** 1）在一定程度上**分离了内核空间和用户空间，保护系统本身的运行安全**；2）可以**减少读盘的次数**，从而提高性能。

**缓存I/O的缺点：** 在缓存 I/O 机制中，**DMA 方式可以将数据直接从磁盘读到页缓存中**，或者将数据从页缓存直接写回到磁盘上，而**不能直接在应用程序地址空间和磁盘之间进行数据传输**。
这样，数据在传输过程中**需要在应用程序地址空间（用户空间）和缓存（内核空间）之间进行多次数据拷贝操作，这些数据拷贝操作所带来的CPU以及内存开销是非常大**的。

所以有了直接IO.
直接IO就是**应用程序直接访问磁盘数据，而不经过内核缓冲区**，**这样做的目的是减少一次从内核缓冲区到用户程序缓存的数据复制**。

**直接I/O的缺点**:就是**如果访问的数据不在应用程序缓存中，那么每次数据都会直接从磁盘进行加载，这种直接加载会非常缓慢**。通常直接I/O跟异步I/O结合使用会得到较好的性能。

### 内存映射(减少数据在用户空间和内核空间之间的拷贝操作,适合大量数据传输)

**内存映射**是指**将硬盘上文件的位置与进程逻辑地址空间中一块大小相同的区域一一对应**，当要访问内存中一段数据时，转换为访问文件的某一段数据。

**这种方式的目的同样是减少数据在用户空间和内核空间之间的拷贝操作。当大量数据需要传输的时候，采用内存映射方式去访问文件会获得比较好的效率。**

通过**mmap()系统调用来建立内存和磁盘文件的关联**，然后像访问内存一样自由地访问文件。

有两种类型的内存映射，**共享型和私有型**，

前者**可以将任何对内存的操作都同步到磁盘文件，而且所有映射同一个文件的进程都共享任意一个进程对映射内存的修改**；

后者**映射的文件只能是只读文件**，所以不可以将对内存的修改同步到文件，而且多个进程不共享修改。

显然，共享型内存映射的效率偏低，因为如果一个文件被很多进程映射，那么每次的修改同步将花费一定的开销。

### 零拷贝

普通的网络传输步骤如下：
1）操作系统将数据从磁盘复制到操作系统内核的页面缓存中
2）应用将数据从内核缓存复制到应用的缓存中
3）应用将数据写回内核的Socket缓存中
4）操作系统将数据从Socket缓存区复制到网卡缓存，然后将其通过网络发出
![alt text](./assets/common_copy.png)

通过**sendfile**传送文件只需要一次系统调用，当调用 sendfile时：
1、首先通过DMA copy将数据从磁盘读取到kernel buffer中
2、然后通过CPU copy将**数据从kernel buffer copy到sokcet buffer**中
3、最终通过DMA copy将socket buffer中数据copy到网卡buffer中发送
sendfile与read/write方式相比，**少了 一次模式切换一次CPU copy**。但是**从上述过程中也可以发现从kernel buffer中将数据copy到socket buffer是没必要的**。
![alt text](./assets/zero_copy1.png)

为此，Linux2.4内核对sendfile做了改进，下图所示
![alt text](./assets/zero_copy2.png)

改进后的处理过程如下：
1、DMA copy将磁盘数据copy到kernel buffer中。
2、向socket buffer中追加当前要发送的数据在kernel buffer中的位置和偏移量。
3、DMA gather copy根据socket buffer中的位置和偏移量直接将kernel buffer中的数据copy到网卡上。

经过上述过程，数据只经过了2次copy就从磁盘传送出去了。（**事实上这个Zero copy是针对内核来讲的，数据在内核模式下是Zero－copy的**）。
当前许多高性能http server都引入了sendfile机制，如nginx，lighttpd等。

### 阻塞 I/O

```cpp
ssize_t read(int fd, void *buf, size_t count);
ssize_t write(int fd, const void *buf, size_t count);
```

* 阻塞调用是指调用结果返回之前，当前线程会被（睡眠）挂起。调用线程只有在得到结果之后才会返回。

二者称为阻塞式系统调用（blocking system calls），因为程序调用 这些函数时会进入 sleep 状态，然后被调度出去（让出处理器），直到 I/O 操作完成：

* 如果数据在文件中，并且文件内容已经缓存在 **page cache** 中，调用会立即返回；
* 如果数据在另一台机器上，就需要通过网络（例如 TCP）获取，会阻塞一段时间；
* 如果数据在硬盘上，也会阻塞一段时间。

但很容易想到，随着存储设备越来越快，程序越来越复杂， 阻塞式（blocking）已经这种最简单的方式已经不适用了。

使用场景：
单线程环境

### 非阻塞式 I/O

进程把一个套接字设置成非阻塞是在**通知内核：当所请求的I/O操作非得把本进程投入睡眠才能完成时，不要把本进程投入睡眠，而是返回一个错误。**

当一个应用进程像这样对一个非阻塞描述符循环调用recvfrom时，我们称之为**轮询 （polling）**。应用进程持续轮询内核，以查看某个操作是否就绪。这么做往往耗费大量CPU时间，

使用场景：
**适用于多线程环境**，数据读写时不会阻塞线程，但需要编写复杂的代码逻辑来判断数据是否读写完成。

### 异步IO

使用场景：

**适用于高并发场景**，不需要阻塞线程，当数据读写完成时会回调指定的函数，因此不需要编写复杂的代码逻辑。

告知内核启动某个操作，并让内核在整个操作（包括将数据从内核复制到我们自己的缓冲区）完成后通知我们。

### I/O多路复用：select()/poll()/epoll()

使用场景：
**适用于高并发场景**，可以**同时监听多个文件描述符**，当其中任意一个可读或可写时就会通知程序，因此不需要频繁切换上下文。

阻塞式之后，出现了一些新的、非阻塞的系统调用，例如 select()、poll() 以及更新的 epoll()。 应用程序在调用这些函数读写时不会阻塞，而是立即返回，返回的是一个 已经 ready 的文件描述符列表。

但这种方式存在一个致命缺点：**只支持 network sockets 和 pipes —— epoll() 甚至连 storage files 都不支持。**

并没有完全的非阻塞
**内核会告诉我文件描述符“已准备好读取”，但我仍然必须调用阻塞 read() 来获取数据，如果我想读取兆字节，很明显这会阻塞。**

**同步 I/O操作** （synchronous I/O opetation）**导致请求进程阻塞**，直到I/O操作完成；
**异步 I/O操作**（asynchronous I/O opetation）**不导致请求进程阻塞**

### 线程池

对于 storage I/O，经典的解决思路是 thread pool： 主线程将 I/O 分发给 worker 线程，后者代替主线程进行阻塞式读写，主线程不会阻塞。

这种方式的问题是线程上下文切换开销可能非常大，后面性能压测会看到。

### Direct I/O（数据库软件）：绕过 page cache

**数据库软件**（database software） 有时 并不想使用操作系统的 **page cache**， 而是希望打开一个文件后，**直接从设备读写这个文件**（direct access to the device）。 这种方式称为**直接访问（direct access）或直接 I/O（direct I/O）**

* 需要指定 O_DIRECT flag；
* 需要应用自己管理自己的缓存 —— 这正是数据库软件所希望的；
* 是 zero-copy I/O，因为应用的缓冲数据直接发送到设备，或者直接从设备读取。

### io_uring

1.**在设计上是真正异步的（truly asynchronous）**。只要 设置了合适的 flag，它在**系统调用上下文中就只是将请求放入队列**， 不会做其他任何额外的事情，**保证了应用永远不会阻塞**。

2.支持任何类型的 I/O：cached files、direct-access files 甚至 blocking sockets。

由于设计上就是异步的（async-by-design nature），因此**无需 poll+read/write 来处理 sockets**。 只需提交一个阻塞式读（blocking read），请求完成之后，就会出现在 completion ring。

3.**灵活、可扩展**：基于 io_uring 甚至能重写（re-implement）Linux 的每个系统调用。

<https://arthurchiao.art/blog/intro-to-io-uring-zh/>
<https://stackoverflow.com/questions/13407542/is-there-really-no-asynchronous-block-i-o-on-linux>

#### 原理及核心数据结构：SQ/CQ/SQE/CQE

## Reactor和Proactor

### 单Reactor/线程

![a](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%8D%95%E8%BF%9B%E7%A8%8B.png)

* Reactor 对象通过 select （IO 多路复用接口） 监听事件，收到事件后通过 dispatch 进行分发，具体分发给 Acceptor 对象还是 Handler 对象，还要看收到的事件类型；
* 如果是连接建立的事件，则交由 Acceptor 对象进行处理，Acceptor 对象会通过 accept 方法 获取连接，并创建一个 Handler 对象来处理后续的响应事件；
* 如果不是连接建立事件， 则交由当前连接对应的 Handler 对象来进行响应；
* Handler 对象通过 read -> 业务处理 -> send 的流程来完成完整的业务流程。

问题：

* 第一个缺点，因为只有一个进程，无法充分利用 多核 CPU 的性能；
* 第二个缺点，Handler 对象在业务处理时，整个进程是无法处理其他连接的事件的，如果业务处理耗时比较长，那么就造成响应的延迟；

所以，单 Reactor 单进程的方案不适用计算机密集型的场景，**只适用于业务处理非常快速的场景。**

### 单 Reactor 多线程 / 多进程

![s](https://cdn.xiaolincoding.com/gh/xiaolincoder/ImageHost4@main/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/Reactor/%E5%8D%95Reactor%E5%A4%9A%E7%BA%BF%E7%A8%8B.png)

单 Reator 多线程的方案优势在于**能够充分利用多核 CPU 的性能**，那既然引入多线程，那么自然就带来了多线程竞争资源的问题。

问题：

**因为一个 Reactor 对象承担所有事件的监听和响应，而且只在主线程中运行，在面对瞬间高并发的场景时，容易成为性能的瓶颈的地方。**

### 多 Reactor 多进程 / 线程

大名鼎鼎的两个开源软件 Netty 和 Memcache 都采用了「多 Reactor 多线程」的方案。

Nginx，不过方案与标准的多 Reactor 多进程有些差异。

具体差异表现在主进程中仅仅用来初始化 socket，并没有创建 mainReactor 来 accept 连接，而是由子进程的 Reactor 来 accept 连接，**通过锁来控制一次只有一个子进程进行 accept（防止出现惊群现象）**，子进程 accept 新连接后就放到自己的 Reactor 进行处理，不会再分配给其他子进程。

### proactor

## socket通信涉及的协议栈
